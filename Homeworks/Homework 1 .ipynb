{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ä°nci Burcu Emeksiz Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) How would you define Machine Learning?\n",
    "\n",
    "- Machine learning is modeling by inferring from data learned through features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
    "\n",
    "Supervised and unsupervised are a learning way in machine learning.\n",
    "\n",
    "- Supervised Learning : It is a type of learning in which data inputs and outputs are trained together for a purpose. Some algorithms used are decision trees, linear regression, neural networks.\n",
    "\n",
    "- Unsupervised Learning : In unsupervised learning model, no output is given to the data. The model should learn on its own according to feauture through inputs.Some algorithms used are random forest, cluster analysis ( k-means, anomaly detection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What are the test and validation set, and why would you want to use them?\n",
    "- Validationset: It is a data set that we can use while we tuning the hyperparameters of the data received to be trained.Test dataset is the dataset that we can try independently after adjusting the hyperparameters.\n",
    "- Thanks to the validation data set, we can objectively evaluate the model we are trying to establish and its hyper parameters. After determining the hyper parameters, we cannot try our model on that set because the data set has learned and become biased. For this reason, we need a data set that we can independently test the final version of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "- As a first step, libraries to be used (numpy, pandas, seaborn) are imported. Each library has a different feature. NumPy includes the basics for data analysis with python. Pandas library is open source. Easy to use data structures and analysis tools. Seaborn is used in data visualization.\n",
    "- In the second step, we import the dataset using pandas.\n",
    "- In the third step, we check whether there is missing data in our data set using isna and isnull. According to our data set, we can fill the empty spaces with the mean or the median. After that we can look for the Outliers.\n",
    "- Fourth step, we change the categorical data. For example, if bad is specified as normal and good in our data set, we convert them to numerical data. Bad = 1 normal = 2, like)\n",
    "- Fifth stage, we divide our dataset into two different sets, training and testing. (Usually 70/30). The reason we split is that in the training data set we use algorithms to test our model. Until we find the best model, our dataset will learn the model. That's why we need an independent rating scale.\n",
    "- At the last stage, we scale the features. In this way, we can learn important and not very effective features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) How you can explore and analyse continuonus and discrete variables?\n",
    "- When we look at the graph of our data set, we can understand which of the distribution of our data is (discrete or continuous). Discrete Variables are countable and have no other values in between. They cannot be considered as continuous variables and there is always some other data in between. For example, the height of individuals in a family or the years they were born."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it\n",
    "- The graph is a histogram. The histogram is used to show the distribution of data ongoing or over a certain time interval. When we examine the table, we can say that the data have continuous variables. The preprocessing processes I gave in my answer above are done. Missing data should be filled in with median values, since they do not show normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
